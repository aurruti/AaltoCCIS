{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scanpath Predictive Model --- EyeFormer\n",
    "### CODE USED FOR ASSIGNMENT A3a\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "# Table of contents\n",
    "* <a href='#1.'> 1. Model overview </a>\n",
    "* <a href='#2.'> 2. Preparations </a>\n",
    "* <a href='#3.'> 3. Inference </a>\n",
    "* <a href='#4.'> 4. Visualisation of results </a>\n",
    "* <a href='#5.'> 5. Evaluation methods </a>\n",
    "\n",
    "\n",
    "<a href='#T1'><b>Student Task 1.</b> Scanpath Prediction </a>\n",
    "\n",
    "    \n",
    "<a href='#T2'><b>Student Task 2.</b> Evaluation metrics </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYG7IQu9VP2V",
    "tags": []
   },
   "source": [
    "\n",
    "## 1. Model overview <a id='1.'></a>\n",
    "\n",
    "\n",
    "<img src=\"imgs/img1.png\">\n",
    "\n",
    "**EyeFormer**  is a model that combines Transformer and Reinforcement Learning to predict scanpaths in Free-Viewing Tasks. The goal of the model is to simulate and predict the sequence of human gaze points while viewing an image or video, and to simulate and predict human scanpaths in Free-Viewing Tasks. The model is able to capture the global information in the image and continuously optimise the prediction strategy through reinforcement learning to achieve highly accurate line-of-sight prediction and saliency prediction.\n",
    "\n",
    "\n",
    "<img src=\"imgs/img2.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generic scanpath prediction\n",
    "Generic scanpath prediction means that the model predicts common eye movement patterns when confronted with a particular image or scene based on the visual behaviour of most observers. This prediction method is critical to understanding which areas attract more attention and how to design more attractive interfaces.\n",
    "\n",
    "In EyeFormer, generic prediction is done through the Transformer model. This model captures global information from the image and continuously optimises its prediction strategy through reinforcement learning. The result is an accurate prediction that shows the areas that most people are most likely to focus on and their order of gaze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Personalised scanpath prediction\n",
    "\n",
    "Unlike generic prediction, personalised scanpath prediction focuses on predicting an individual's unique eye movement patterns based on their visual behavioural characteristics. Each individual may have a different focus and order of attention when looking at an image, and EyeFormer is able to generate predictions that match the behaviour of a specific individual by learning a small amount of scan path data about that individual.\n",
    "\n",
    "<img src=\"imgs/img3.png\">\n",
    "\n",
    "Personalised predictions are important in many real-world applications. For example, in custom user interface design, we need to ensure that the interface layout adapts to the visual habits of a particular user to enhance their experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mcj_AbU1VP2W",
    "tags": []
   },
   "source": [
    "## 2. Preparations <a id='2.'></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1.  Environment preparation\n",
    "First, make sure you have installed the necessary Python packages and environment. The following code will help you create and configure the environment needed to run the EyeFormer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Sdf35adtNi2",
    "outputId": "0b078239-ca09-4247-c435-7a7656b45274",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qq ruamel.yaml==0.17.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "btSItrlhstOg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import ruamel.yaml as yaml\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dkZHo55qvCaZ",
    "outputId": "22484c9b-91f6-4243-e115-75c97a371bec",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install -q pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 cudatoolkit==11.0\n",
    "!pip install -q opencv-python==4.5.3.56 Pillow einops multimatch-gaze\n",
    "!pip install -q transformers==4.8.1\n",
    "!pip install -q timm==0.4.9\n",
    "\n",
    "%cd EyeFormer-UIST2024\n",
    "\n",
    "# Download the weights file (Generic scanpath prediction weights file)\n",
    "!pip install gdown\n",
    "!gdown https://drive.google.com/uc?id=1n2l7leXJqAM16TZnlpMiw1N-jeY7Bi4K -O ./weights/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYjiRznmxLtj"
   },
   "source": [
    "### Step 2. Loading and configuring the model\n",
    "\n",
    "**NOTE:**\n",
    "If you have your own eval images. Please change the `eval_image_root` string in `configs/Tracking.yaml` to your eval image dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'EyeFormer-UIST2024'\n",
      "/notebooks/compdesign2024/Visual_Saliency/EyeFormer-UIST2024\n"
     ]
    }
   ],
   "source": [
    "from models.model_tracking import TrackingTransformer\n",
    "from models.vit import interpolate_pos_embed\n",
    "\n",
    "import utils\n",
    "from dataset import create_dataset, create_sampler, create_loader\n",
    "import csv\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import ruamel.yaml as yaml\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "%cd EyeFormer-UIST2024\n",
    "from models.model_tracking import TrackingTransformer\n",
    "from models.vit import interpolate_pos_embed\n",
    "\n",
    "import utils\n",
    "from dataset import create_dataset, create_sampler, create_loader\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines some of the parameters needed for the model to run and loads the specific configuration from the YAML file. Make sure that the checkpoint path points correctly to the pre-trained model file.\n",
    "\n",
    "Create a configuration class **ARGS** and load the configuration file to initialise the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EXfTMtBDstOh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ARGS:\n",
    "    def __init__(self, config):\n",
    "        self.config = './configs/Tracking.yaml'\n",
    "        self.checkpoint = './weights/checkpoint_19.pth'\n",
    "        self.resume = False\n",
    "        self.output_dir = 'output/tracking_eval'\n",
    "        self.text_encoder = 'bert-base-uncased'\n",
    "        self.device ='cpu'\n",
    "        self.seed = 42\n",
    "        self.world_size = 1\n",
    "        self.dist_url = 'env://'\n",
    "        self.distributed = True\n",
    "        \n",
    "\n",
    "from ruamel.yaml import YAML        \n",
    "        \n",
    "args = ARGS(config = './configs/Tracking.yaml')\n",
    "\n",
    "yaml = YAML(typ='rt')\n",
    "config = yaml.load(open(args.config, 'rt'))\n",
    "\n",
    "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "yaml.dump(config, open(os.path.join(args.output_dir, 'config.yaml'), 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model inference <a id='3.'></a>\n",
    "Use the EyeFormer model for inference to generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a **test** function for loading data and making inferences. This function will step through each image during the test, predict the scan path, and save the results to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KXu75aZPstOh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(model, data_loader, tokenizer, device, output_dir, config):\n",
    "    # train\n",
    "    model.eval()\n",
    "\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Testing:'\n",
    "\n",
    "    image_names = []\n",
    "    results = []\n",
    "    widths = []\n",
    "    heights = []\n",
    "    # user_ids = []\n",
    "    for i, (image, image_name, width, height) in enumerate(metric_logger.log_every(data_loader, 1, header)):\n",
    "        image = image.to(device, non_blocking=True)\n",
    "        # user_id = user_id.to(device, non_blocking=True)\n",
    "\n",
    "        coord = model.inference(image)\n",
    "        coord = coord.detach().cpu().numpy().tolist()\n",
    "\n",
    "        width = width.numpy().tolist()\n",
    "        height = height.numpy().tolist()\n",
    "\n",
    "        # user_id = user_id.cpu().numpy().tolist()\n",
    "\n",
    "        image_names.extend(image_name)\n",
    "        results.extend(coord)\n",
    "        widths.extend(width)\n",
    "        heights.extend(height)\n",
    "        # user_ids.extend(user_id)\n",
    "\n",
    "    with open(os.path.join(output_dir, 'predicted_result.csv'), 'w') as wfile:\n",
    "        writer = csv.writer(wfile)\n",
    "        writer.writerow([\"image\", \"width\", \"height\", \"x\", \"y\", \"timestamp\"])\n",
    "\n",
    "        for image, width, height, coord in zip(image_names, widths, heights, results):\n",
    "\n",
    "            for row in coord:\n",
    "                x = row[0] * width\n",
    "                y = row[1] * height\n",
    "                t = row[2]\n",
    "                # username = data_loader.dataset.id2user[user_id]\n",
    "                writer.writerow([image, width, height,\n",
    "                                x, y, t])\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model loading and data inference in the **main** function. The main function will load the model, dataset and call the test function for inference. The inference results will be saved to the specified output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aGtJPjU8stOh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(args, config):\n",
    "    # utils.init_distributed_mode(args)\n",
    "\n",
    "    device = torch.device(args.device)\n",
    "\n",
    "    # fix the seed for reproducibility\n",
    "    seed = args.seed + utils.get_rank()\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    #### Dataset ####\n",
    "    print(\"Creating dataset\")\n",
    "    datasets = [create_dataset('inference', config)]\n",
    "\n",
    "    if False:\n",
    "        num_tasks = utils.get_world_size()\n",
    "        global_rank = utils.get_rank()\n",
    "        samplers = create_sampler(datasets, [True], num_tasks, global_rank)\n",
    "    else:\n",
    "        samplers = [None]\n",
    "\n",
    "    data_loader = create_loader(datasets,\n",
    "                                samplers,\n",
    "                                batch_size=[config['batch_size_test']],\n",
    "                                num_workers=[16],\n",
    "                                is_trains=[False],\n",
    "                                collate_fns=[None])[0]\n",
    "\n",
    "    # tokenizer = BertTokenizer.from_pretrained(args.text_encoder)\n",
    "    tokenizer = None\n",
    "\n",
    "    #### Model ####\n",
    "    print(\"Creating model\")\n",
    "    model = TrackingTransformer(config=config, init_deit=False)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    if args.checkpoint:\n",
    "        checkpoint = torch.load(args.checkpoint, map_location='cpu')\n",
    "        state_dict = checkpoint['model']\n",
    "\n",
    "        msg = model.load_state_dict(state_dict)\n",
    "        print('load checkpoint from %s' % args.checkpoint)\n",
    "        print(msg)\n",
    "\n",
    "    model_without_ddp = model\n",
    "\n",
    "    print(\"Start testing\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    test(model, data_loader, tokenizer, device, args.output_dir, config)\n",
    "\n",
    "    # dist.barrier()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Testing time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HQTLur4YstOh",
    "outputId": "d0d9ec57-e2e8-4d6d-b9ce-1e2204b44b5a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset\n",
      "Creating model\n",
      "Model will generate 16 points\n",
      "load checkpoint from ./weights/checkpoint_19.pth\n",
      "<All keys matched successfully>\n",
      "Start testing\n",
      "Testing:  [0/7]  eta: 0:00:41    time: 5.9185  data: 0.8545\n",
      "Testing:  [1/7]  eta: 0:00:19    time: 3.2791  data: 0.4298\n",
      "Testing:  [2/7]  eta: 0:00:11    time: 2.3823  data: 0.2883\n",
      "Testing:  [3/7]  eta: 0:00:07    time: 1.9310  data: 0.2162\n",
      "Testing:  [4/7]  eta: 0:00:04    time: 1.6621  data: 0.1734\n",
      "Testing:  [5/7]  eta: 0:00:02    time: 1.4813  data: 0.1448\n",
      "Testing:  [6/7]  eta: 0:00:01    time: 1.3518  data: 0.1242\n",
      "Testing: Total time: 0:00:09 (1.3828 s / it)\n",
      "Testing time 0:00:09\n"
     ]
    }
   ],
   "source": [
    "## run main function to get prediction file.\n",
    "\n",
    "main(args, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Visualization of results <a id='4.'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following visualisation code allows you to display the **ground truth and prediction results** on the image separately or simultaneously. You can choose which results to display as desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we run the following cell to prepare the functions and libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os, random, csv\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def extract_scanpaths(csvfile):\n",
    "    res = []\n",
    "    cur = None\n",
    "    obj = None\n",
    "\n",
    "    with open(csvfile) as f:\n",
    "        reader = csv.DictReader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            uniq = row['image']\n",
    "\n",
    "            if uniq != cur:\n",
    "                if 'obj' in locals() and cur is not None:\n",
    "                    res.append(obj)\n",
    "\n",
    "                obj = {'image': row['image'], 'width': int(row['width']), 'height': int(row['height']),\n",
    "                       'scanpath': [], 'duration': []}\n",
    "\n",
    "            x, y, t = float(row['x']), float(row['y']), float(row['timestamp'])\n",
    "            obj['scanpath'].append([x, y])\n",
    "            obj['duration'].append(t)\n",
    "\n",
    "            cur = uniq\n",
    "\n",
    "    if len(obj['scanpath']) > 0:\n",
    "        res.append(obj)\n",
    "\n",
    "    return res\n",
    "\n",
    "def collect_info(res, dir, method):\n",
    "    images = []\n",
    "    xs_list = []\n",
    "    ys_list = []\n",
    "    ts_list = []\n",
    "    methods = []\n",
    "\n",
    "    for r in res:\n",
    "        image_path = os.path.join(dir, r[\"image\"])\n",
    "        images.append(image_path)\n",
    "        scan_path = r[\"scanpath\"]\n",
    "        xs = [p[0] for p in scan_path]\n",
    "        ys = [p[1] for p in scan_path]\n",
    "        xs_list.append(xs)\n",
    "        ys_list.append(ys)\n",
    "        ts_list.append(r['duration'])\n",
    "        methods.append(method)\n",
    "\n",
    "    return images, xs_list, ys_list, ts_list, methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is the definition of the *visuliaztion* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize(data_type):\n",
    "    \n",
    "    \n",
    "    if data_type == \"gt\":        \n",
    "        output_dir = '/notebooks/compdesign2024/Visual_Saliency/scanpath_visualization_output/gt'\n",
    "        \n",
    "    elif data_type == \"pred\":        \n",
    "        output_dir = '/notebooks/compdesign2024/Visual_Saliency/scanpath_visualization_output/pred'\n",
    "        \n",
    "    elif data_type == \"comb\":\n",
    "        output_dir = '/notebooks/compdesign2024/Visual_Saliency/scanpath_visualization_output/comb'\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Invalid data_type. Use 'gt', 'pred' or 'comb'.\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    data_dir = \"/notebooks/compdesign2024/Visual_Saliency/imgs/test\"\n",
    "    gt_file = \"/notebooks/compdesign2024/Visual_Saliency/EyeFormer-UIST2024/evaluation/testing_ground_truth.csv\"\n",
    "    pred_file = \"/notebooks/compdesign2024/Visual_Saliency/EyeFormer-UIST2024/output/tracking_eval/predicted_result.csv\"\n",
    "\n",
    "    gt_res = extract_scanpaths(gt_file)\n",
    "    pred_res = extract_scanpaths(pred_file)\n",
    "\n",
    "    g_images, g_xs_list, g_ys_list, g_ts_list, g_methods = collect_info(gt_res, data_dir, \"gt\")\n",
    "    p_images, p_xs_list, p_ys_list, p_ts_list, p_methods = collect_info(pred_res, data_dir, \"pred\")    \n",
    "    \n",
    "    # Combine ground truth and projected results data\n",
    "    images = list(set(g_images + p_images))\n",
    "    image_to_gt = {img: (g_xs_list[g_images.index(img)], g_ys_list[g_images.index(img)], g_ts_list[g_images.index(img)]) for img in g_images if img in g_images}\n",
    "    image_to_pred = {img: (p_xs_list[p_images.index(img)], p_ys_list[p_images.index(img)], p_ts_list[p_images.index(img)]) for img in p_images if img in p_images}\n",
    "    # Define the colour mapping\n",
    "    cm_gt = plt.get_cmap('winter_r')\n",
    "    cm_pred = plt.get_cmap('autumn_r')\n",
    "    \n",
    "  \n",
    "\n",
    "\n",
    "  \n",
    "    \n",
    "    \n",
    "    for image in images:\n",
    "        try:\n",
    "            img = Image.open(image).convert(\"RGB\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {image}\")\n",
    "            continue\n",
    "\n",
    "        img.putalpha(int(255 * 0.8))\n",
    "        img = np.array(img)\n",
    "\n",
    "        width = img.shape[1]\n",
    "        height = img.shape[0]\n",
    "        plt.gray()\n",
    "        plt.axis('off')\n",
    "        ax = plt.imshow(img)\n",
    "\n",
    "        \n",
    "        \n",
    "        ########################################################################## Mapping ground truth\n",
    "                    \n",
    "        if data_type == \"gt\":\n",
    "            \n",
    "            # Mapping ground truth\n",
    "            if image in image_to_gt:\n",
    "                xs_gt, ys_gt, ts_gt = image_to_gt[image]\n",
    "                cmap_gt = (cm_gt(np.linspace(0, 1, 2 * len(xs_gt) - 1)) * 255).astype(np.uint8)\n",
    "                for i in range(len(xs_gt)):\n",
    "                    if i > 0:\n",
    "                        ax.axes.arrow(\n",
    "                            xs_gt[i - 1],\n",
    "                            ys_gt[i - 1],\n",
    "                            (xs_gt[i] - xs_gt[i - 1]),\n",
    "                            (ys_gt[i] - ys_gt[i - 1]),\n",
    "                            width=min(width, height) / 300 * 3,\n",
    "                            head_width=0.05,\n",
    "                            head_length=0.01,\n",
    "                            color=cmap_gt[i * 2 - 1] / 255.,\n",
    "                            alpha=1,\n",
    "                        )\n",
    "                for i in range(len(xs_gt)):\n",
    "                    edgecolor = 'red' if i == 0 else 'black'\n",
    "                    circle = plt.Circle(\n",
    "                        (xs_gt[i], ys_gt[i]),\n",
    "                        radius=min(width, height) / 35 * ts_gt[i] * 2 * 1.1 * 1.5,\n",
    "                        edgecolor=edgecolor,\n",
    "                        facecolor=cmap_gt[i * 2] / 255.,\n",
    "                        linewidth=1\n",
    "                    )\n",
    "                    ax.axes.add_patch(circle)\n",
    "                      \n",
    "                    \n",
    "        ######################################################################### Mapping prediction result          \n",
    "        \n",
    "                \n",
    "        \n",
    "        if data_type == \"pred\":    \n",
    "            \n",
    "            # Mapping prediction result\n",
    "            if image in image_to_pred:\n",
    "                xs_pred, ys_pred, ts_pred = image_to_pred[image]\n",
    "                cmap_pred = (cm_pred(np.linspace(0, 1, 2 * len(xs_pred) - 1)) * 255).astype(np.uint8)\n",
    "                for i in range(len(xs_pred)):\n",
    "                    if i > 0:\n",
    "                        ax.axes.arrow(\n",
    "                            xs_pred[i - 1],\n",
    "                            ys_pred[i - 1],\n",
    "                            (xs_pred[i] - xs_pred[i - 1]),\n",
    "                            (ys_pred[i] - ys_pred[i - 1]),\n",
    "                            width=min(width, height) / 300 * 3,\n",
    "                            head_width=0.05,\n",
    "                            head_length=0.01,\n",
    "                            color=cmap_pred[i * 2 - 1] / 255.,\n",
    "                            alpha=1,\n",
    "                        )\n",
    "                for i in range(len(xs_pred)):\n",
    "                    edgecolor = 'blue' if i == 0 else 'black'\n",
    "                    circle = plt.Circle(\n",
    "                        (xs_pred[i], ys_pred[i]),\n",
    "                        radius=min(width, height) / 35 * ts_pred[i] * 2 * 1.1 * 1.5,\n",
    "                        edgecolor=edgecolor,\n",
    "                        facecolor=cmap_pred[i * 2] / 255.,\n",
    "                        linewidth=1\n",
    "                    )\n",
    "                    ax.axes.add_patch(circle)\n",
    "\n",
    "                    \n",
    "        ########################################################################### Mapping both (combination)         \n",
    "        \n",
    "        \n",
    "        if data_type == \"comb\":\n",
    "            \n",
    "            # Mapping ground truth\n",
    "            if image in image_to_gt:\n",
    "                xs_gt, ys_gt, ts_gt = image_to_gt[image]\n",
    "                cmap_gt = (cm_gt(np.linspace(0, 1, 2 * len(xs_gt) - 1)) * 255).astype(np.uint8)\n",
    "                for i in range(len(xs_gt)):\n",
    "                    if i > 0:\n",
    "                        ax.axes.arrow(\n",
    "                            xs_gt[i - 1],\n",
    "                            ys_gt[i - 1],\n",
    "                            (xs_gt[i] - xs_gt[i - 1]),\n",
    "                            (ys_gt[i] - ys_gt[i - 1]),\n",
    "                            width=min(width, height) / 300 * 3,\n",
    "                            head_width=0.05,\n",
    "                            head_length=0.01,\n",
    "                            color=cmap_gt[i * 2 - 1] / 255.,\n",
    "                            alpha=1,\n",
    "                        )\n",
    "                for i in range(len(xs_gt)):\n",
    "                    edgecolor = 'red' if i == 0 else 'black'\n",
    "                    circle = plt.Circle(\n",
    "                        (xs_gt[i], ys_gt[i]),\n",
    "                        radius=min(width, height) / 35 * ts_gt[i] * 2 * 1.1 * 1.5,\n",
    "                        edgecolor=edgecolor,\n",
    "                        facecolor=cmap_gt[i * 2] / 255.,\n",
    "                        linewidth=1\n",
    "                    )\n",
    "                    ax.axes.add_patch(circle)\n",
    "            \n",
    "            # Mapping prediction result\n",
    "            if image in image_to_pred:\n",
    "                xs_pred, ys_pred, ts_pred = image_to_pred[image]\n",
    "                cmap_pred = (cm_pred(np.linspace(0, 1, 2 * len(xs_pred) - 1)) * 255).astype(np.uint8)\n",
    "                for i in range(len(xs_pred)):\n",
    "                    if i > 0:\n",
    "                        ax.axes.arrow(\n",
    "                            xs_pred[i - 1],\n",
    "                            ys_pred[i - 1],\n",
    "                            (xs_pred[i] - xs_pred[i - 1]),\n",
    "                            (ys_pred[i] - ys_pred[i - 1]),\n",
    "                            width=min(width, height) / 300 * 3,\n",
    "                            head_width=0.05,\n",
    "                            head_length=0.01,\n",
    "                            color=cmap_pred[i * 2 - 1] / 255.,\n",
    "                            alpha=1,\n",
    "                        )\n",
    "                for i in range(len(xs_pred)):\n",
    "                    edgecolor = 'blue' if i == 0 else 'black'\n",
    "                    circle = plt.Circle(\n",
    "                        (xs_pred[i], ys_pred[i]),\n",
    "                        radius=min(width, height) / 35 * ts_pred[i] * 2 * 1.1 * 1.5,\n",
    "                        edgecolor=edgecolor,\n",
    "                        facecolor=cmap_pred[i * 2] / 255.,\n",
    "                        linewidth=1\n",
    "                    )\n",
    "                    ax.axes.add_patch(circle)\n",
    "\n",
    "        #########################################################################################                  \n",
    "                    \n",
    "\n",
    "        imagename = os.path.basename(image).split(\".\")[0]\n",
    "        ax.figure.savefig(output_dir + \"/\" + '{}_{}.png'.format(imagename, data_type), dpi=120, bbox_inches=\"tight\")\n",
    "        plt.close(ax.figure)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a visualization for our desired analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def create_display(background_path, sticker_paths, coordinates, sizes):\n",
    "    # Open the background image\n",
    "    background = Image.open(background_path).convert(\"RGBA\")\n",
    "    bg_width, bg_height = background.size\n",
    "\n",
    "    # Loop through each sticker image\n",
    "    for i, sticker_path in enumerate(sticker_paths):\n",
    "        sticker = Image.open(sticker_path).convert(\"RGBA\")\n",
    "        max_size = int(sizes[i].item() if isinstance(sizes[i], torch.Tensor) else sizes[i])\n",
    "\n",
    "        aspect_ratio = sticker.width / sticker.height\n",
    "        if sticker.width > sticker.height:\n",
    "            new_width = max_size\n",
    "            new_height = int(max_size / aspect_ratio)\n",
    "        else:\n",
    "            new_height = max_size\n",
    "            new_width = int(max_size * aspect_ratio)\n",
    "\n",
    "        sticker = sticker.resize((new_width, new_height), Image.LANCZOS)\n",
    "        \n",
    "        # so that the corrdinates refer to the centerpoint of each sticker\n",
    "        x = int(coordinates[i][0] * bg_width / 100) - new_width // 2\n",
    "        y = int(coordinates[i][1] * bg_height / 100) - new_height // 2\n",
    "        \n",
    "        background.paste(sticker, (x, y), sticker)\n",
    "\n",
    "    return background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Optimization Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset\n",
      "Creating model\n",
      "Model will generate 16 points\n",
      "load checkpoint from ./weights/checkpoint_19.pth\n",
      "<All keys matched successfully>\n",
      "Start testing\n",
      "Testing:  [0/7]  eta: 0:00:42    time: 6.1150  data: 4.8567\n",
      "Testing:  [1/7]  eta: 0:00:20    time: 3.4554  data: 2.4321\n",
      "Testing:  [2/7]  eta: 0:00:12    time: 2.5229  data: 1.6214\n",
      "Testing:  [3/7]  eta: 0:00:08    time: 2.0598  data: 1.2168\n",
      "Testing:  [4/7]  eta: 0:00:05    time: 1.7798  data: 0.9735\n",
      "Testing:  [5/7]  eta: 0:00:03    time: 1.5929  data: 0.8113\n",
      "Testing:  [6/7]  eta: 0:00:01    time: 1.4604  data: 0.6957\n",
      "Testing: Total time: 0:00:10 (1.5004 s / it)\n",
      "Testing time 0:00:10\n"
     ]
    }
   ],
   "source": [
    "output_path = \"/notebooks/compdesign2024/Visual_Saliency/imgs/A3a/\"\n",
    "output_path_backup = \"/notebooks/compdesign2024/Visual_Saliency/imgs/test/\"\n",
    "scanpath_csv_path = \"/notebooks/compdesign2024/Visual_Saliency/EyeFormer-UIST2024/output/tracking_eval/predicted_result.csv\"\n",
    "\n",
    "background_path = \"/notebooks/compdesign2024/Visual_Saliency/imgs/background.png\"\n",
    "sticker_paths = [\"/notebooks/compdesign2024/Visual_Saliency/imgs/button1.png\", \"/notebooks/compdesign2024/Visual_Saliency/imgs/button2.png\", \"/notebooks/compdesign2024/Visual_Saliency/imgs/button3.png\"]\n",
    "coordinates = [(52, 50), (70, 80), (30, 80)]    # All in %!\n",
    "sizes = [100, 100, 100]                           # All in %!\n",
    "\n",
    "\n",
    "image = create_display(background_path, sticker_paths, coordinates, sizes)\n",
    "image.save(output_path + \"1.png\")\n",
    "image.save(output_path_backup + \"1.png\")\n",
    "\n",
    "coordinates = [(52, 50), (70, 70), (30, 80)]    # All in %!\n",
    "sizes = [100, 100, 100]                           # All in %!\n",
    "           \n",
    "image = create_display(background_path, sticker_paths, coordinates, sizes)\n",
    "image.save(output_path + \"2.png\")\n",
    "image.save(output_path_backup + \"2.png\")\n",
    "\n",
    "coordinates = [(52, 50), (70, 70), (30, 80)]    # All in %!\n",
    "sizes = [100, 120, 80]                           # All in %!\n",
    "\n",
    "image = create_display(background_path, sticker_paths, coordinates, sizes)\n",
    "image.save(output_path + \"3.png\")\n",
    "image.save(output_path_backup + \"3.png\")\n",
    "\n",
    "           \n",
    "main(args, config)\n",
    "visualize(\"pred\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "97007115a572f64893389487481285c3",
     "grade": false,
     "grade_id": "cell-b70f734137b9d858",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "tags": []
   },
   "source": [
    "Ignore this block. This is for Maunal Grading for your report"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
